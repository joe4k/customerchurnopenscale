{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Batch Scoring Notebook\nIn this notebook, we step through batch scoring against a deployed machine learning model. There are multiple uses for batch scoring but our focus in this scenario is batch scoring so that Watson Openscale can collect enough data points to be able to provide adequate metrics on fairness and accuracy of the trained model.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import pandas as pd"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient"
        }, 
        {
            "source": "## WML Deployed Model\nIn this section, we setup the configuration to access Watson Machine Learning deployed model.\n\nIn the next cell, please provide the Watson Machine Learning credentials which you can get by navigating to your IBM Cloud account, finding your Watson Machine Learning service, and clicking on \"Service Credentials\".\n```\nwml_credentials = {\n    \"apikey\": \"########\",\n    \"instance_id\": \"#########\",\n    \"url\": \"##########\"\n}\n```", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Fill out apikey, instance_id, and url values from your WML service credentials\nwml_credentials = {\n    \"apikey\": \"#########\",\n    \"instance_id\": \"##########\",\n    \"url\": \"#########\"\n}"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "client = WatsonMachineLearningAPIClient( wml_credentials )"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "client.deployments.list()"
        }, 
        {
            "source": "client.deployments.delete('##########')", 
            "cell_type": "raw", 
            "metadata": {}
        }, 
        {
            "source": "#### Identify the right deployment id from the above list corresponding to the deployment to which you want to send the scoring request. \n#### Use the same deployment id to get deployment details in the next cell", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "deployment_id = '##################'\ndeployment_details = client.deployments.get_details(deployment_id)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Extract the scoring end point for the churn prediction model\nchurn_scoring_url = deployment_details['entity']['scoring_url']\nprint(churn_scoring_url)"
        }, 
        {
            "source": "## You can run this cell if you want to do a quick test with one record\nfields = ['LONGDISTANCE', 'INTERNATIONAL', 'LOCAL', 'DROPPED', 'PAYMETHOD', 'LOCALBILLTYPE', 'LONGDISTANCEBILLTYPE', 'USAGE', 'RATEPLAN', 'GENDER', 'STATUS', 'CHILDREN', 'ESTINCOME', 'CAROWNER', 'AGE']\nvalues = [\n    [23, 0, 206, 0, 'CC', 'Budget', 'Intnl_discount', 229, 3, 'F', 'S', 1, 38000.0, 'N', 24.393]\n]\ntest_data = {\n    \"fields\": fields,\n    \"values\": values\n}\nresult = client.deployments.score(churn_scoring_url, test_data )\nprint(result)\n", 
            "cell_type": "raw", 
            "metadata": {}
        }, 
        {
            "source": "## Scoring Data\nIn this section, we download the scoring data which we will send to the deployed customer churn prediction machine learning model.\n\nThe data is stored as a csv file on github so it can be loaded directly into a pandas dataframe using read_csv functionality.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "csvurl = \"https://raw.githubusercontent.com/joe4k/customerchurnopenscale/master/data/CustomerChurnScoringData.csv\"\nchurn_data_daily = pd.read_csv(csvurl,header=0)\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "churn_data_daily.shape"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# drop ID column\nchurnDF = churn_data_daily.drop(['ID'],axis=1)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "churnDF.head()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "deployment_details = client.deployments.get_details(deployment_id)\nchurn_scoring_url = deployment_details['entity']['scoring_url']\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "flds = churnDF.columns.tolist()\nprint(flds)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Use only the first 30 records. In practice, you want to monitor all scoring results but for this hands-on lab, 30 records is sufficient to emulate fairness monitor in Openscale.\nsample = churnDF[0:30]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Print top records\nsample.head()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Print size/shape of the sample\nsample.shape"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "values = sample.values.tolist()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "flds = sample.columns.tolist()\nvalues = sample.values.tolist()\npayload = {\n    \"fields\": flds,\n    \"values\": values\n}"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "result = client.deployments.score(churn_scoring_url, payload)\nprint(result)"
        }, 
        {
            "source": "## Conclusion\nYou have completed running batch scoring task for monitoring a churn prediction machine learning model.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.6.8", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}