{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At first let us ingest the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Click the red '1010' icon in the right top of the toolbar. There customer_usage_history.csv will appear. Click on th arrow at the right of Insert to Code. And then select Insert Spark DataFrame in Python. This will insert the dataframe in the next cell. Change the name of the dataframe (in both theplaces) to customer_usage_history and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-------------+-----+-------+---------+-------------+--------------------+-----+--------+-----+\n",
      "| ID|LONGDISTANCE|INTERNATIONAL|LOCAL|DROPPED|PAYMETHOD|LOCALBILLTYPE|LONGDISTANCEBILLTYPE|USAGE|RATEPLAN|CHURN|\n",
      "+---+------------+-------------+-----+-------+---------+-------------+--------------------+-----+--------+-----+\n",
      "|  1|          23|            0|  206|      0|       CC|       Budget|      Intnl_discount|  229|       3|    T|\n",
      "|  6|          29|            0|   45|      0|       CH|    FreeLocal|            Standard|   75|       2|    F|\n",
      "|  8|          24|            0|   22|      0|       CC|    FreeLocal|            Standard|   47|       3|    F|\n",
      "| 11|          26|            0|   32|      1|       CC|       Budget|            Standard|   59|       1|    F|\n",
      "| 17|          12|            0|   46|      4|       CC|    FreeLocal|            Standard|   58|       1|    F|\n",
      "+---+------------+-------------+-----+-------+---------+-------------+--------------------+-----+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SQLContext\n",
    "# Add asset from file system\n",
    "customer_usage_history = SQLContext(sc).read.csv(os.environ['DSX_PROJECT_DIR']+'/datasets/customer_usage_history.csv', header='true', inferSchema = 'true')\n",
    "customer_usage_history.show(5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the able step for the seconf dataset - customer_demograhic_history.csv. This time change the name of the dataframe to customer_demograhic_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+--------+---------+--------+---------+\n",
      "| ID|GENDER|STATUS|CHILDREN|ESTINCOME|CAROWNER|      AGE|\n",
      "+---+------+------+--------+---------+--------+---------+\n",
      "|  1|     F|     S|       1|  38000.0|       N|24.393333|\n",
      "|  6|     M|     M|       2|  29616.0|       N|49.426667|\n",
      "|  8|     M|     M|       0|  19732.8|       N|50.673333|\n",
      "| 11|     M|     S|       2|    96.33|       N|56.473333|\n",
      "| 17|     M|     M|       2|  53010.8|       N|    18.84|\n",
      "+---+------+------+--------+---------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Add asset from file system\n",
    "customer_demographic_history = SQLContext(sc).read.csv(os.environ['DSX_PROJECT_DIR']+'/datasets/customer_demographic_history.csv', header='true', inferSchema = 'true')\n",
    "customer_demographic_history.show(5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let us create 2 temporary views from these two dataframes so that we can join these two table to get the merged dataset. We shall join these two datasets using the column 'ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_usage_history.createOrReplaceTempView(\"custusagehistT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_demographic_history.createOrReplaceTempView(\"custdemohistT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we shall join these two views using the column ID. That will create the dataframe that we can use for building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhacced_customer_history = \\\n",
    "SQLContext(sc).sql(\"select a.*, b.GENDER, b.STATUS, b.CHILDREN, b.ESTINCOME, b.CAROWNER, b.AGE from custusagehistT a,custdemohistT b where a.ID = b.ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us create another temporary view out if this merged dataframe and see how many records we have with Churn Flag 'Y' and how many with Churn Flaf 'N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhacced_customer_history.createOrReplaceTempView(\"enhancedcustT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|CHURN|churnClount|\n",
      "+-----+-----------+\n",
      "|    F|        838|\n",
      "|    T|        577|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SQLContext(sc).sql(\"select CHURN, count(CHURN) as churnClount from enhancedcustT group by CHURN\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let us save this merged datafrme as a CSV file. This file we shall read back in next notebook to build tyhe model. Please note that the file saved this way would be saved in spark format where the main data would be saved as a part file. However while reading the data back in Spark you can use the same name that you have used to save the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhacced_customer_history.coalesce(1).write.csv(os.environ['DSX_PROJECT_DIR']+'/datasets/enhanced_customers.csv', header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
