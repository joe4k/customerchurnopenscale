{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this Notebook we shall create a Machine Learning Model using Spark and Save the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The key reason we are using PySpark here is to show how to create a Machine Learning Model using large volume of data that may not fit within the memory of single Python process. So we need distributed computing infrastructure for those cases. We are using Spark here as a distributed computing Infrastructure. PySpark is a library that helps using Spark's capability using Python as programming language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we import PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"WOSM\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we read in a dataset that we will use to develop a Machine Learning model. Once we read in the data, it can potentially reside on multiple machines/VMs in a distributed fashion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can read the data here in various ways. We are showing here two mechanisms - reading data from a CSV file and reading data from a Database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you are reading the data from CSV file, read the data you have stored in previous step using the same file name in the next cell. Otherwise skip the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-------------+-----+-------+---------+-------------+--------------------+-----+--------+-----+------+------+--------+---------+--------+---------+\n",
      "| ID|LONGDISTANCE|INTERNATIONAL|LOCAL|DROPPED|PAYMETHOD|LOCALBILLTYPE|LONGDISTANCEBILLTYPE|USAGE|RATEPLAN|CHURN|GENDER|STATUS|CHILDREN|ESTINCOME|CAROWNER|      AGE|\n",
      "+---+------------+-------------+-----+-------+---------+-------------+--------------------+-----+--------+-----+------+------+--------+---------+--------+---------+\n",
      "|  1|          23|            0|  206|      0|       CC|       Budget|      Intnl_discount|  229|       3|    T|     F|     S|       1|  38000.0|       N|24.393333|\n",
      "|  6|          29|            0|   45|      0|       CH|    FreeLocal|            Standard|   75|       2|    F|     M|     M|       2|  29616.0|       N|49.426667|\n",
      "|  8|          24|            0|   22|      0|       CC|    FreeLocal|            Standard|   47|       3|    F|     M|     M|       0|  19732.8|       N|50.673333|\n",
      "| 11|          26|            0|   32|      1|       CC|       Budget|            Standard|   59|       1|    F|     M|     S|       2|    96.33|       N|56.473333|\n",
      "| 17|          12|            0|   46|      4|       CC|    FreeLocal|            Standard|   58|       1|    F|     M|     M|       2|  53010.8|       N|    18.84|\n",
      "+---+------------+-------------+-----+-------+---------+-------------+--------------------+-----+--------+-----+------+------+--------+---------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmergedDf = spark.read.csv(os.environ['DSX_PROJECT_DIR']+'/datasets/enhanced_customers.csv', header='true', inferSchema = 'true')\n",
    "cmergedDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you are reading the data from the Database read the data from the same using the instruction provided in the Hands On Lab document. Otherwise skip the next 2 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------+---------+------+---+------+-----+-------+-------------+-----+-------------+------------+--------------------+---------+--------+-----+\n",
      "|      AGE|CAROWNER|CHILDREN|ESTINCOME|GENDER| ID|STATUS|CHURN|DROPPED|INTERNATIONAL|LOCAL|LOCALBILLTYPE|LONGDISTANCE|LONGDISTANCEBILLTYPE|PAYMETHOD|RATEPLAN|USAGE|\n",
      "+---------+--------+--------+---------+------+---+------+-----+-------+-------------+-----+-------------+------------+--------------------+---------+--------+-----+\n",
      "|24.393333|       N|       1|  38000.0|     F|  1|     S|    T|      0|            0|  206|       Budget|          23|      Intnl_discount|       CC|       3|  229|\n",
      "|49.426667|       N|       2|  29616.0|     M|  6|     M|    F|      0|            0|   45|    FreeLocal|          29|            Standard|       CH|       2|   75|\n",
      "|50.673333|       N|       0|  19732.8|     M|  8|     M|    F|      0|            0|   22|    FreeLocal|          24|            Standard|       CC|       3|   47|\n",
      "|56.473333|       N|       2|    96.33|     M| 11|     S|    F|      1|            0|   32|       Budget|          26|            Standard|       CC|       1|   59|\n",
      "|    18.84|       N|       2|  53010.8|     M| 17|     M|    F|      4|            0|   46|    FreeLocal|          12|            Standard|       CC|       1|   58|\n",
      "+---------+--------+--------+---------+------+---+------+-----+-------+-------------+-----+-------------+------------+--------------------+---------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dsx_core_utils, requests, os, io\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Add asset from remote connection\n",
    "df83 = None\n",
    "dataSet = dsx_core_utils.get_remote_data_set_info('USER1009.CUSTOMER_MERGED_HISTORY_VIEW')\n",
    "dataSource = dsx_core_utils.get_data_source_info(dataSet['datasource'])\n",
    "sparkSession = SparkSession(sc).builder.getOrCreate()\n",
    "# Load JDBC data to Spark dataframe\n",
    "dbTableOrQuery = '\"' + (dataSet['schema'] + '\".\"' if(len(dataSet['schema'].strip()) != 0) else '') + dataSet['table'] + '\"'\n",
    "if (dataSet['query']):\n",
    "    dbTableOrQuery = \"(\" + dataSet['query'] + \") TBL\"\n",
    "df83 = sparkSession.read.format(\"jdbc\").option(\"url\", dataSource['URL']).option(\"dbtable\", dbTableOrQuery).option(\"user\",dataSource['user']).option(\"password\",dataSource['password']).load()\n",
    "df83.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the previous cell in last line there would be show() command for the dataset that created. In the next cell change the name of the dataset to cmergedDf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment next line. Use the dataframe name used in the last line of previous cell (for the show() command) instead of df81. Then execute this cell.\n",
    "#cmergedDf = df82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With this data we can create a Binary Classification model that can predict whether a person is a top KoL or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- LONGDISTANCE: integer (nullable = true)\n",
      " |-- INTERNATIONAL: integer (nullable = true)\n",
      " |-- LOCAL: integer (nullable = true)\n",
      " |-- DROPPED: integer (nullable = true)\n",
      " |-- PAYMETHOD: string (nullable = true)\n",
      " |-- LOCALBILLTYPE: string (nullable = true)\n",
      " |-- LONGDISTANCEBILLTYPE: string (nullable = true)\n",
      " |-- USAGE: integer (nullable = true)\n",
      " |-- RATEPLAN: integer (nullable = true)\n",
      " |-- CHURN: string (nullable = true)\n",
      " |-- GENDER: string (nullable = true)\n",
      " |-- STATUS: string (nullable = true)\n",
      " |-- CHILDREN: integer (nullable = true)\n",
      " |-- ESTINCOME: double (nullable = true)\n",
      " |-- CAROWNER: string (nullable = true)\n",
      " |-- AGE: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmergedDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In next 2 lines we create a distributed temporary table/view out of the Dataset so that we can access the data easily using SQL syntax (through use of Spark SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmergedDf.createOrReplaceTempView(\"mergedt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "custDf = spark.sql(\"select *, case when AGE < 1 then 'INFANT' when AGE < 18 then 'Child' else 'Adult' End as PHASE from mergedt\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1415"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custDf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For developing the Binary Classification Model we want to use Spark ML Lib. Spark MLLib has implementations of various machine learning algorithms that can help creating model on Distributed datasets. Using Spark MLLib, one can create a model using dataset as big as needed for developing a real life Machine Learning model as long as the data is available in the Distributed dataset we created in previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we import components from Spark MLLib for developing the model using the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are importing Pipeline, as that helps building the model development steps as pipeline and the same can be applied to training dataset easily. Additionally we are also importing the VectorAssembler that helps in asembling the attributes (the ones we want to use as features of the model) as Vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, IndexToString\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare string variables so that they can be used by the decision tree algorithm\n",
    "# StringIndexer encodes a string column of labels to a column of label indices\n",
    "SI1 = StringIndexer(inputCol='GENDER', outputCol='GenderEncoded')\n",
    "SI2 = StringIndexer(inputCol='STATUS',outputCol='StatusEncoded')\n",
    "SI3 = StringIndexer(inputCol='CAROWNER',outputCol='CarOwnerEncoded')\n",
    "SI4 = StringIndexer(inputCol='PAYMETHOD',outputCol='PaymethodEncoded')\n",
    "SI5 = StringIndexer(inputCol='LOCALBILLTYPE',outputCol='LocalBilltypeEncoded')\n",
    "SI6 = StringIndexer(inputCol='LONGDISTANCEBILLTYPE',outputCol='LongDistanceBilltypeEncoded')\n",
    "SI7 = StringIndexer(inputCol='PHASE',outputCol='PhaseEncoded')\n",
    "S18 = StringIndexer(inputCol='CHURN', outputCol='label')\n",
    "\n",
    "# Pipelines API requires that input variables are passed in  a vector\n",
    "assembler = VectorAssembler(inputCols=[\"GenderEncoded\", \"StatusEncoded\", \"CarOwnerEncoded\", \"PaymethodEncoded\", \"LocalBilltypeEncoded\", \\\n",
    "                                       \"LongDistanceBilltypeEncoded\", \"PhaseEncoded\", \"CHILDREN\", \"ESTINCOME\", \"AGE\", \"LONGDISTANCE\", \"INTERNATIONAL\", \"LOCAL\",\\\n",
    "                                      \"DROPPED\",\"USAGE\",\"RATEPLAN\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = S18.fit(custDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use the Random Forest algorithm for Binary Classification. One could also use another supervised learning algorithm such as Binary Treee or XG Boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the algorithm, take the default settings\n",
    "rf=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we define a pipeline with 2 stages. In first stage the features are assembled and in the next stage they are used to create a model using the Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,SI5,SI6,SI7,labelIndexer, assembler, rf, labelConverter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, LONGDISTANCE: int, INTERNATIONAL: int, LOCAL: int, DROPPED: int, PAYMETHOD: string, LOCALBILLTYPE: string, LONGDISTANCEBILLTYPE: string, USAGE: int, RATEPLAN: int, CHURN: string, GENDER: string, STATUS: string, CHILDREN: int, ESTINCOME: double, CAROWNER: string, AGE: double, PHASE: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into train and test datasets\n",
    "train, test = custDf.randomSplit([0.7,0.3], seed=6)\n",
    "train.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In next step the Model gets created when we apply the pipeline on the Training dataset. It returns the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build models\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we test the model by using the Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, LONGDISTANCE: int, INTERNATIONAL: int, LOCAL: int, DROPPED: int, PAYMETHOD: string, LOCALBILLTYPE: string, LONGDISTANCEBILLTYPE: string, USAGE: int, RATEPLAN: int, CHURN: string, GENDER: string, STATUS: string, CHILDREN: int, ESTINCOME: double, CAROWNER: string, AGE: double, PHASE: string, GenderEncoded: double, StatusEncoded: double, CarOwnerEncoded: double, PaymethodEncoded: double, LocalBilltypeEncoded: double, LongDistanceBilltypeEncoded: double, PhaseEncoded: double, label: double, features: vector, rawPrediction: vector, probability: vector, prediction: double, predictedLabel: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CHURN</th>\n",
       "      <th>label</th>\n",
       "      <th>predictedLabel</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.8201269331036564, 0.17987306689634358]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.9021391200887166, 0.09786087991128342]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.5969959289050152, 0.4030040710949847]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.9252897734458483, 0.07471022655415167]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>T</td>\n",
       "      <td>1.0</td>\n",
       "      <td>T</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.16958321269672788, 0.8304167873032722]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>38</td>\n",
       "      <td>T</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.7682895609145737, 0.2317104390854264]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID CHURN  label predictedLabel  prediction  \\\n",
       "0   6     F    0.0              F         0.0   \n",
       "1  17     F    0.0              F         0.0   \n",
       "2  22     F    0.0              F         0.0   \n",
       "3  24     F    0.0              F         0.0   \n",
       "4  29     T    1.0              T         1.0   \n",
       "5  38     T    1.0              F         0.0   \n",
       "\n",
       "                                 probability  \n",
       "0  [0.8201269331036564, 0.17987306689634358]  \n",
       "1  [0.9021391200887166, 0.09786087991128342]  \n",
       "2   [0.5969959289050152, 0.4030040710949847]  \n",
       "3  [0.9252897734458483, 0.07471022655415167]  \n",
       "4  [0.16958321269672788, 0.8304167873032722]  \n",
       "5   [0.7682895609145737, 0.2317104390854264]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.transform(test)\n",
    "results=results.select(results[\"ID\"],results[\"CHURN\"],results[\"label\"],results[\"predictedLabel\"],results[\"prediction\"],results[\"probability\"])\n",
    "results.toPandas().head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision model = 0.93.\n"
     ]
    }
   ],
   "source": [
    "print('Precision model = {:.2f}.'.format(results.filter(results.label == results.prediction).count() / float(results.count())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we import  the BinaryClassificationEvaluator object from Spark ML Lib for evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We evaluate the model using the area under the Receiver Operating Characteristic (ROC) curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC curve = 0.93.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "print('Area under ROC curve = {:.2f}.'.format(evaluator.evaluate(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we save the Model. Please note the URL that is crated after teh model is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsx_ml.ml import save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Telco Churn Model 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'path': '/user-home/1001/DSX_Projects/CustomerChurnDemo/models/Telco Churn Model 2/1',\n",
       " 'scoring_endpoint': 'https://dsxl-api/v3/project/score/Python36/spark-2.3/CustomerChurnDemo/Telco%20Churn%20Model%202/1'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save(name = model_name, model = model, algorithm_type = \"Classification\", test_data = test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
